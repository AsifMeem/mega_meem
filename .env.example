# LLM Provider: "gemini", "anthropic", or "ollama"
LLM_PROVIDER=gemini

# Gemini settings (default provider)
GEMINI_API_KEY=your-gemini-api-key
GEMINI_MODEL=gemini-2.0-flash

# Anthropic settings (alternative provider)
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Ollama settings (local inference â€” private, free)
# OLLAMA_MODEL=llama3.2:8b
# OLLAMA_BASE_URL=http://localhost:11434

# Database
DATABASE_PATH=./data/future_asif.db
